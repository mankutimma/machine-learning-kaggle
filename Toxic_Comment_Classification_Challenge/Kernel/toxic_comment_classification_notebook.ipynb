{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d493a15b-c3f8-4fc8-b6f6-b244262cc9cc",
    "_uuid": "f5e86b7bf4f117737ae32952c22d6ada16b45b5c"
   },
   "source": [
    "**Perform the necessary imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "5e3d88fb-79b6-4876-aa4f-700ee70b6a97",
    "_uuid": "68cb2423a6f01d469434af0430887b537efcfb51",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de245cc0-77b6-41e1-95eb-a1c262a67d13",
    "_uuid": "36fe75d8b240981c941a1f0c67831acbb5b5b944"
   },
   "source": [
    "**Necessary global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9ee07ff6-a9f6-43e6-acd5-0c0b6991434c",
    "_uuid": "2b0d3e0df7b39c957a5aa82929828c6d38ed045c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "max_features = 20000\n",
    "max_text_length = 400\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "297811ae-23ee-471a-b3c1-a39afda0603b",
    "_uuid": "f7c54e5252d6eaa902043414368ecf077a7122c4"
   },
   "source": [
    "**Quick peek into the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d30f5899-23a4-42a1-aa44-bb0f4e075104",
    "_uuid": "c7ce7a4f881105d1d64fe22b9197311e057696e3",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                       comment_text  toxic  \\\n",
      "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
      "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
      "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
      "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
      "4  79357270  The reader here is not going by my say so for ...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "672807f9-6ce2-47d1-9a1c-80fe07576b09",
    "_uuid": "4cae0fb90301fc9a5ed4005b498261eb939c213e"
   },
   "source": [
    "**Printing using 'iloc' just for fun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2b90735a-eb0a-4ee3-9545-2233f2fdee02",
    "_uuid": "25e62320d15cacbc201714c861591c18d2acc220",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\n",
      "Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\n"
     ]
    }
   ],
   "source": [
    "print(train_df.iloc[0, -7])\n",
    "print(train_df.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "858757df-0cb8-4187-8c3d-1172ba673af4",
    "_uuid": "d107221dc682c12682832224bfe5cc6fd10462a9"
   },
   "source": [
    "**Checking if  NaNs exist in the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3a1ffb98-08b4-4363-b19f-d4b4ed6c808a",
    "_uuid": "8a9fcc9505de340c9f0a157b2c9aef37851e7b91",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(pd.isnull(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "752260bc-592e-40c1-8d4a-728873fdbe47",
    "_uuid": "6e3afb7d148fcba97b16dfa69898ede9d83edf01"
   },
   "source": [
    "**Apparently no NaNs in the training set!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b1145e3-caa3-474d-b471-0fa37f402801",
    "_uuid": "0a76a762fce842e1d254d84d44af56cd6d33333e"
   },
   "source": [
    "**Converting pandas series to a numpy array using .values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "a74097c1-ba58-45c1-ae57-551496a96994",
    "_uuid": "aaa6ccda53c3facedcd6f92e5e5f7408c6a28efc",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\"\n",
      " '\"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"'\n",
      " '\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the \"\"points of interest\"\" section you added because it seemed kind of spammy. I know you probably didn\\'t mean to disobey the rules, but generally, a point of interest tends to be rather touristy, and quite irrelevant to an area culture. That\\'s just my opinion, though.\\n\\nIf you want to reply, just put your reply here and add {{talkback|Jamiegraham08}} on my talkpage.   \"'\n",
      " ...,\n",
      " 'Mamoun Darkazanli\\nFor some reason I am unable to fix the bold formatting on the Arabic name of Mamoun Darkazanli. The entire first paragraph is bolded and I just want to bold the script. Please take a look when you get the chance.'\n",
      " 'Salafi would be a better term. It is more politically correct to use in Islam. Very few Muslims, even those who do not agree with those sects, use the term Islamist. The proper term used in Islamic circles is salafi, if anything. Sometime wahabi, but that is considered to be deragatory by some.'\n",
      " 'making wikipedia a better and more inviting place.']\n"
     ]
    }
   ],
   "source": [
    "x = train_df['comment_text'].values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8263e42d-a76b-4e3b-bfb1-c42fe366f7bb",
    "_uuid": "bb2b3f733ffee73d9504542814ceda82282f7007",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties of x\n",
      "type : <class 'numpy.ndarray'>, dimensions : 1, shape : (95851,), total no. of elements : 95851, data type of each element: object, size of each element 8 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"properties of x\")\n",
    "print(\"type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes\".format(type(x), x.ndim, x.shape, x.size, x.dtype, x.itemsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88890c4f-bfd1-47f4-bc69-30435001c6f5",
    "_uuid": "7abe952d6a9b178edadc0751795649842c4ff042"
   },
   "source": [
    "**Getting the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "706f8df5-41b1-4a75-9d20-864b5cf3d791",
    "_uuid": "7b65845c54b983d2602e68f1e0438b9c712dc300",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ..., \n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "y = train_df[list_of_classes].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "1a7180e9-0756-4f58-bb57-d37dde2bd528",
    "_uuid": "5fc08696b61be737b0b1f16b412abed4c5a2abb9",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties of y\n",
      "type : <class 'numpy.ndarray'>, dimensions : 2, shape : (95851, 6), total no. of elements : 575106, data type of each element: int64, size of each element 8 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"properties of y\")\n",
    "print(\"type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes\".format(type(y), y.ndim, y.shape, y.size, y.dtype, y.itemsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af34631e-ff2c-4cf3-bee5-0b5a9bdb0db9",
    "_uuid": "b8ba64cd620fc3453bce297105fcdb8d6fb8e306"
   },
   "source": [
    "**Keras makes our life easy. Using Tokenizer to get a list of sequence and then padding it form a 2D numpy array **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "55cf2aa3-b53e-4f4e-b9f1-f4043f04120d",
    "_uuid": "e8ed34f540de09e56dcd95bc0e509d4a406eb8a8",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x7f70241c6278>\n",
      "<keras.preprocessing.text.Tokenizer object at 0x7f70241c6278>\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print(x_tokenizer)\n",
    "x_tokenizer.fit_on_texts(list(x))\n",
    "print(x_tokenizer)\n",
    "x_tokenized = x_tokenizer.texts_to_sequences(x) #list of lists(containing numbers), so basically a list of sequences, not a numpy array\n",
    "#pad_sequences:transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape \n",
    "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "758e64cc-2646-45bf-b9e1-ace946e8174b",
    "_uuid": "6ad16f7b64b4cd930ba3d2837937d9ad9548bb84",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties of x_train_val\n",
      "type : <class 'numpy.ndarray'>, dimensions : 2, shape : (95851, 400), total no. of elements : 38340400, data type of each element: int32, size of each element 4 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"properties of x_train_val\")\n",
    "print(\"type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes\".format(type(x_train_val), x_train_val.ndim, x_train_val.shape, x_train_val.size, x_train_val.dtype, x_train_val.itemsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd3693ba-3872-4e07-96f9-99087ae5bd70",
    "_uuid": "d68eea53d70a1701ba939991fdeb9976c8df05c4"
   },
   "source": [
    "**90% of the data is used for training and the rest for validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d84418e1-95e7-4024-b494-825e00ce8172",
    "_uuid": "4bdd0b3298393be4f5150c725569dadb789fb3b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a044595d-de8f-4d4d-b547-5c1fee4e4e4f",
    "_uuid": "2848de4a72693329cdd114c3e2b0d5fe2e5c4cd3"
   },
   "source": [
    "**Start building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "c66cbdb4-9c9d-4120-aed9-023f2d5c1659",
    "_uuid": "56417e8ca623249048c89be6ba75b18209abb6da",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:381: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1506      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,102,006\n",
      "Trainable params: 1,102,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=max_text_length))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto 6 output layers, and squash it with a sigmoid:\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67a54a4c-f08a-490c-ae9f-e7a13c7e41e1",
    "_uuid": "b2aabd44e7161e983ddea65a6489fb9b18e187e9"
   },
   "source": [
    "**Begin training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "5ce38563-9ce9-4c5c-b084-350a5a74bc0b",
    "_uuid": "297008f72969696c9031cbdfa7eb2ed964a2069a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86265/86265 [==============================] - 493s 6ms/step - loss: 0.0732 - acc: 0.9762 - val_loss: 0.0568 - val_acc: 0.9796\n",
      "Epoch 2/2\n",
      "86265/86265 [==============================] - 503s 6ms/step - loss: 0.0479 - acc: 0.9821 - val_loss: 0.0568 - val_acc: 0.9797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7095399940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1fbbc91d-73fd-4ed4-8c66-9fbdec9485d9",
    "_uuid": "f03aed757872dac28edd054ccf956d9249ade471"
   },
   "source": [
    "**Good job! 98% accuracy on the validation set. Scope for improvement exists!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb35da05-2661-42d2-be40-6656eda6ff71",
    "_uuid": "3592c6fa0c6100d8847db8ab7c53a9b85d6a48b1"
   },
   "source": [
    "**Quick peek into the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "f99b26ea-0ee9-4ea6-aa71-8e4e3a3d5cda",
    "_uuid": "038b1acf1b88e71ef0b5cd29070fd9bfa90c9c95",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                       comment_text\n",
      "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
      "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
      "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
      "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
      "4  22982444                      == [WIKI_LINK: Talk:Celts] ==\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/test.csv')\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96fe1fc4-cbc6-45df-98f9-79fa8fa9326f",
    "_uuid": "1b903303556d9efa8716153b453f5c19860c00e3"
   },
   "source": [
    "**Checking if  NaNs exist in the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "4ee9acc5-131f-4aee-b42d-7dba412338ee",
    "_uuid": "441dab5b1c9a842943d6d0f37537ae79b12e5868",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([52300]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(pd.isnull(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "219da36e-9c01-4cc5-bb37-b60dcf828e1d",
    "_uuid": "17003afb11c9a45651e950a999410d40dad13e10"
   },
   "source": [
    "**Hmmm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "0b578f37-57ea-4163-9544-7f98c51e5c43",
    "_uuid": "fac1694d92d39700287ea83c2f56aa1d38eb5315",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[52300, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0bb52e9e-209c-44a3-b798-e03498a8fc54",
    "_uuid": "1d8461f41d14fe71577fa337d0666232181d5b9c"
   },
   "source": [
    "**Fill the NaN field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "f940ba4b-0381-4662-8c2d-ad2c4684dd14",
    "_uuid": "8ab3002d00c86fda5802a7546d722b62fc3bcfc2",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['==Orphaned non-free media (Image:41cD1jboEvL. SS500 .jpg)=='\n",
      " '::Kentuckiana is colloquial.  Even though the area is often referred to as this, it (in my opinion) has never held the encyclopedic precision of \"Louisville metropolitian area\", which has a specific U.S. Census definition.  Also, apparently Kentuckiana often refers to the local television viewing area, which isn\\'t nearly contiguous with the official metro area.  As you indicate, Kentuckiana seems to be more of a slang or marketing phenomena than anything we could pin down in encyclopedic terms here.  That\\'s why we see Wikipedia language like \"the Louisville metropolitan area, sometimes referred to as Kentuckiana\". That\\'s my take on it. —   •'\n",
      " 'Hello fellow Wikipedians,\\nI have just modified  on [WIKI_LINK: Double Trouble (George Jones and Johnny Paycheck album)]. Please take a moment to review [EXTERNAL_LINK: my edit]. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit  for additional information. I made the following changes:\\n*Added archive [EXTERNA_LINK: https://web.archive.org/web/20070403204132/http://www.epiccenter.com/] to [EXTERNA_LINK: http://www.epiccenter.com/]\\nWhen you have finished reviewing my changes, please set the checked parameter below to true or failed to let others know (documentation at ).\\nCheers.—'\n",
      " ...,\n",
      " '==Fair use rationale for [WIKI_LINK: Image:D.R.I. - Definition.jpg]=='\n",
      " '== Employment Practices at Majestic =='\n",
      " 'Welcome to Wikipedia. Although everyone is welcome to make constructive contributions to Wikipedia, at least one of your recent edits, such as the one you made to [WIKI_LINK: INevel2], did not appear to be constructive and has been automatically [WIKI_LINK: Help:Reverting@reverted] by .  Please use [WIKI_LINK: Wikipedia:Sandbox@the sandbox] for any test edits you would like to make, and take a look at the [WIKI_LINK: Wikipedia:Introduction@welcome page] to learn more about contributing to this encyclopedia.  If you believe there has been a mistake and would like to report a false positive, please  and then remove this warning from your talk page.  If your edit was not vandalism, please feel free to make your edit again after reporting it. The following is the log entry regarding this warning: [WIKI_LINK: INevel2] was [EXTERNAL_LINK: changed] by    blanking the page on 2007-12-25T19:53:51+00:00 . Thank you.']\n"
     ]
    }
   ],
   "source": [
    "x_test = test_df['comment_text'].fillna('comment_missing').values\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3072aca7-6fd9-4da3-8561-730b9b6b1706",
    "_uuid": "f3b50a49e1a5dbf25ce0cb182db3da60a9db85d9"
   },
   "source": [
    "**Tokenizing and padding similar to what we did before to training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "67e8eb4e-7457-4649-9513-411794e55ada",
    "_uuid": "6c2b27f7d2c0c1f6158b52b965479f1e257a3afc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
    "x_testing = sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ca42dea-b68c-4e31-a3f9-99e415ddc3be",
    "_uuid": "cadb4f9b8a646599eca8d107a2416214b49442ee"
   },
   "source": [
    "**Time to predict!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d1cc833e-46f4-46bf-bbe0-0a39ef80ff5d",
    "_uuid": "d323d8503236d4cf01a3253f6639445ec9735ee9",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 210s 925us/step\n"
     ]
    }
   ],
   "source": [
    "y_testing = model.predict(x_testing, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e25db199-5b05-455f-8e87-b2fd4b7265a0",
    "_uuid": "e3ec631b8e942bb6a3dd843f40d42241f1373308"
   },
   "source": [
    "**Submit predictions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "612d0744-ec1c-40bc-bed6-5028f9936ac6",
    "_uuid": "d87c22acba7dda6524eba4b7fdc6c915b8301174",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_of_classes] = y_testing\n",
    "sample_submission.to_csv(\"toxic_comment_classification.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
